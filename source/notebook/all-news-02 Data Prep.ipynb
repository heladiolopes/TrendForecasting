{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:32.860882Z",
     "start_time": "2020-09-21T21:56:32.854016Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = \"../assets/data/all-news\"\n",
    "\n",
    "splitted_path = os.path.abspath(os.path.join(base_path, \"splitted\"))\n",
    "lemmatized_path = os.path.abspath(os.path.join(base_path, \"lemmatized\"))\n",
    "transaction_path = os.path.abspath(os.path.join(base_path, \"transaction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:33.976124Z",
     "start_time": "2020-09-21T21:56:33.971172Z"
    }
   },
   "outputs": [],
   "source": [
    "# List splitted files\n",
    "splitted_files = os.listdir(splitted_path)\n",
    "splitted_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaner Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:37.777468Z",
     "start_time": "2020-09-21T21:56:37.753419Z"
    }
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import contractions\n",
    "import inflect\n",
    "import string\n",
    "import re\n",
    "\n",
    "def to_lower(sentence: str):\n",
    "    return sentence.lower()\n",
    "\n",
    "def number_to_text(sentence: str):\n",
    "    p = inflect.engine()\n",
    "    numbers = re.finditer(r'\\d+', sentence)\n",
    "    for x in reversed([x for x in numbers]):\n",
    "        number = sentence[x.start():x.end()]\n",
    "\n",
    "        if number.isdigit():\n",
    "            word = p.number_to_words(number)\n",
    "            sentence = sentence[:x.start()] + word + sentence[x.end():]\n",
    "    return sentence\n",
    "\n",
    "def remove_numbers(sentence: str):\n",
    "    return re.sub(r'\\d+', '', sentence)\n",
    "\n",
    "def remove_punctuation(sentence: str, keep_final:bool=False):\n",
    "    PUNCTUATION = string.punctuation\n",
    "    if keep_final:\n",
    "        PUNCTUATION = PUNCTUATION.replace(\".\", \"\")\n",
    "            \n",
    "    translator = str.maketrans('', '', PUNCTUATION)\n",
    "    return sentence.translate(translator)\n",
    "\n",
    "def remove_whitespaces(sentence: str):\n",
    "    return \" \".join(sentence.split())\n",
    "\n",
    "def expand_contractions(sentence: str):\n",
    "    return contractions.fix(sentence)\n",
    "\n",
    "def special_characters_to_ascii(sentence: str):\n",
    "    return unidecode(sentence)\n",
    "\n",
    "def drop_links(sentence: str):\n",
    "    return re.sub(r\"http\\S+\", \"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:39.706298Z",
     "start_time": "2020-09-21T21:56:39.700815Z"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(sentence:str, convert_numbers:bool=True, keep_final:bool=False):\n",
    "    # Drop urls\n",
    "    sentence = drop_links(sentence)\n",
    "\n",
    "    # Treat number converting to text or/and removing\n",
    "    if convert_numbers:\n",
    "        sentence = number_to_text(sentence)\n",
    "    sentence = remove_numbers(sentence)  # if convert mode drop the eventual remain\n",
    "    \n",
    "    # Expand contractions\n",
    "    sentence = expand_contractions(sentence)\n",
    "\n",
    "    # Remove punctuation\n",
    "    sentence = remove_punctuation(sentence, keep_final)\n",
    "\n",
    "    # Convert special characters\n",
    "    sentence = special_characters_to_ascii(sentence)\n",
    "\n",
    "    # Normalizing case\n",
    "    sentence = to_lower(sentence)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    sentence = remove_whitespaces(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remove Stopwords and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:44.455995Z",
     "start_time": "2020-09-21T21:56:42.904545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop words base list\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en\", disable=['parser', 'ner'])\n",
    "\n",
    "def tokenize(sentence: str):\n",
    "#     return gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "    return sentence.split(\" \")\n",
    "\n",
    "def remove_stopwords(tokens, stop_words=stop_words):\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 3]\n",
    "\n",
    "def lemmatization(tokens, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "#     return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "    return [token.lemma_ for token in doc if token.lemma_ != \"-PRON-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other auxiliars functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:46.174734Z",
     "start_time": "2020-09-21T21:56:45.927200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_id(df: pd.DataFrame):\n",
    "    df['id'] = df.index\n",
    "    df['id'] = df['id'].apply(lambda x: '{}/{}'.format(file[:-4], x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T21:56:46.788799Z",
     "start_time": "2020-09-21T21:56:46.782556Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_dataframe(df: pd.DataFrame, filename):   \n",
    "    if os.path.exists(filename):\n",
    "        df.to_csv(filename, index=False, mode='a', header=False)\n",
    "    else:\n",
    "        df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T23:16:49.592901Z",
     "start_time": "2020-09-21T22:00:09.226681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling the file: 2016-01.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "CHUNKSIZE = 1_000\n",
    "\n",
    "for file in splitted_files[:1]:\n",
    "    filename = os.path.join(splitted_path, file)\n",
    "    transactional_fname = os.path.join(transaction_path, file)\n",
    "    full_fname = os.path.join(lemmatized_path, file)\n",
    "    \n",
    "    # Check if files already exists\n",
    "    if os.path.exists(transactional_fname) and os.path.exists(full_fname):\n",
    "        print(\"File already saved:\", file)\n",
    "        continue\n",
    "    \n",
    "    # Read data frame in chunks\n",
    "    chunks = pd.read_csv(\n",
    "        filename, \n",
    "        chunksize=CHUNKSIZE, \n",
    "#         nrows=300\n",
    "    )\n",
    "    \n",
    "    print(\"Handling the file:\", file)\n",
    "    for df in chunks:\n",
    "\n",
    "        # Fill na with empty\n",
    "        df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "        df[\"article\"] = df[\"article\"].fillna(\"\")\n",
    "\n",
    "        # Generate id\n",
    "        df = generate_id(df)   \n",
    "\n",
    "        # Generating transactional data\n",
    "        df[\"article\"] = df[\"article\"].str.split(\".\")\n",
    "\n",
    "        # Add title as a single transaction\n",
    "        df['text'] = df.apply(lambda row: [row['title']] + row['article'], axis=1)\n",
    "        df = df.drop(labels=['title', 'article', 'year_month'], axis=1)\n",
    "\n",
    "        # Expand text columns by lis elements\n",
    "        df = df.explode(\"text\").reset_index(drop=True)\n",
    "\n",
    "        # Clean text columns\n",
    "        df['text'] = df['text'].apply(standardize)\n",
    "\n",
    "        # Tokenize text\n",
    "        df[\"tokens\"] = df['text'].apply(tokenize)\n",
    "\n",
    "        # Lemmatize\n",
    "        df[\"tokens\"] = df[\"tokens\"].apply(lemmatization)\n",
    "\n",
    "        # Removing stop words\n",
    "        df[\"tokens\"] = df[\"tokens\"].apply(remove_stopwords)\n",
    "\n",
    "        # Drop empty transactions\n",
    "        df = df[df[\"tokens\"].apply(len) > 0]\n",
    "\n",
    "        # Put in text again\n",
    "        df[\"text\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "        df = df.drop(labels=['tokens'], axis=1)\n",
    "\n",
    "        # Save transactional data\n",
    "        save_dataframe(df, transactional_fname)\n",
    "        \n",
    "        # Convert to full file and save if\n",
    "        df = df.groupby([\"date\", \"id\"], as_index=False).agg({\"text\": lambda x: \" \".join(list(x))})\n",
    "        save_dataframe(df, full_fname)\n",
    "        \n",
    "        # Clean df for memory saving\n",
    "        del df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T23:32:12.767530Z",
     "start_time": "2020-09-21T23:32:08.974943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.25.13; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\u001b[?25l\u0000\n",
      "Directory: /home/heladio/Downloads/\n",
      "\n",
      "Terminal control enabled, press 'h' for listing of keys and functions.\n",
      "\n",
      "Playing MPEG stream 1 of 1: piseiro.mp3 ...\n",
      "\n",
      "MPEG 1.0 L III cbr256 44100 j-s\n",
      "\n",
      "Title:                                   Artist: @XandAviao                     \n",
      "Comment:                                 Album:  CD Setembro 2020               \n",
      "Year:    2020                            Genre:  Forró                         \n",
      "\n",
      "[0:03] Decoding of piseiro.mp3 finished.\n",
      "\u001b[?25h\u0000"
     ]
    }
   ],
   "source": [
    "!mpg123 /home/heladio/Downloads/piseiro.mp3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
