{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T00:37:59.821384Z",
     "start_time": "2020-09-22T00:37:59.815232Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"../assets/data/all-news\"\n",
    "\n",
    "lemmatized_path = os.path.abspath(os.path.join(base_path, \"lemmatized\"))\n",
    "transaction_path = os.path.abspath(os.path.join(base_path, \"transaction\"))\n",
    "\n",
    "# List files\n",
    "lemmatized_files = os.listdir(lemmatized_path)\n",
    "transaction_files = os.listdir(transaction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:02:40.112968Z",
     "start_time": "2020-09-22T01:02:38.139998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01/0</td>\n",
       "      <td>paris hilton woman black uncle montys funeral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01/0</td>\n",
       "      <td>paris hilton arrive lax wednesday dress pay la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01/0</td>\n",
       "      <td>paris fly switzerland especially funeral brins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01/0</td>\n",
       "      <td>monty die sunday long battle cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01/0</td>\n",
       "      <td>loss obviously hit paris hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0  2016-01/0      paris hilton woman black uncle montys funeral\n",
       "1  2016-01/0  paris hilton arrive lax wednesday dress pay la...\n",
       "2  2016-01/0  paris fly switzerland especially funeral brins...\n",
       "3  2016-01/0                monty die sunday long battle cancer\n",
       "4  2016-01/0                      loss obviously hit paris hard"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(transaction_path, transaction_files[0]), usecols=['id', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency based stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:02:47.655563Z",
     "start_time": "2020-09-22T01:02:47.644418Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_stop_words(data: pd.Series, percentile_top: int = 95, percentile_bottom: int = 5, quiet=False):\n",
    "    t0 = dt.datetime.now()\n",
    "    \n",
    "    # Get unique words for each document\n",
    "    unique = data.apply(lambda x: list(set(x.split())))\n",
    "    \n",
    "    # Calculate the words document frequency\n",
    "    words_document_frequency = unique.explode().value_counts() / len(unique)\n",
    "    \n",
    "    # Get percentiles values\n",
    "    top = np.percentile(words_document_frequency, percentile_top)\n",
    "    bottom = np.percentile(words_document_frequency, percentile_bottom)\n",
    "    \n",
    "    # Find words\n",
    "    mask_top = words_document_frequency >= top\n",
    "    mask_bottom = bottom >= words_document_frequency\n",
    "    mask_wdf = mask_top | mask_bottom\n",
    "    freq_stop_words = list(words_document_frequency[mask_wdf].index)\n",
    "    vocabulary = list(words_document_frequency[~mask_wdf].index)\n",
    "    \n",
    "    if not quiet:\n",
    "        length = len(str(len(words_document_frequency))) + 1\n",
    "        print(f\"{len(words_document_frequency)}\".rjust(length), \"- raw vocabulary length\")\n",
    "        print(f\"{len(vocabulary)}\".rjust(length), \"- new vocabulary length\\n\")\n",
    "        print(f\"{mask_wdf.sum()}\".rjust(length), \"- new stop words founded\")\n",
    "        print(f\"{mask_top.sum()}\".rjust(length), f\"- df above  {top:.8f}\")\n",
    "        print(f\"{mask_bottom.sum()}\".rjust(length), f\"- df bellow {bottom:.8f}\\n\")\n",
    "        print(f\"Max df: {words_document_frequency.max():.8f}\")\n",
    "        print(f\"Min df: {words_document_frequency.min():.8f}\\n\")        \n",
    "        print(f\"Execution in {dt.datetime.now() - t0}\")\n",
    "        \n",
    "    return freq_stop_words, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:03:00.805206Z",
     "start_time": "2020-09-22T01:02:49.145383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 243635 - raw vocabulary length\n",
      " 110819 - new vocabulary length\n",
      "\n",
      " 132816 - new stop words founded\n",
      "  12235 - df above  0.00006010\n",
      " 120581 - df bellow 0.00000077\n",
      "\n",
      "Max df: 0.13734729\n",
      "Min df: 0.00000077\n",
      "\n",
      "Execution in 0:00:11.227079\n"
     ]
    }
   ],
   "source": [
    "# Find stop words\n",
    "freq_stop_words, vocabulary = find_stop_words(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation: Corpus and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T01:03:19.061864Z",
     "start_time": "2020-09-22T01:03:02.359010Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary([[word] for word in vocabulary])\n",
    "\n",
    "# Create Corpus with Term Document Frequency\n",
    "df[\"corpus\"] = df.text.str.split().apply(id2word.doc2bow)\n",
    "\n",
    "# Removing empty documents\n",
    "filterred_df = df[df[\"corpus\"].apply(len) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T03:34:00.660514Z",
     "start_time": "2020-09-22T01:05:51.963476Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(\n",
    "    corpus=filterred_df.corpus.to_list(),\n",
    "    id2word=id2word,\n",
    "    num_topics=10, \n",
    "    random_state=100,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:30:06.696357Z",
     "start_time": "2020-09-22T11:30:06.567563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.008*\"qian\" + 0.008*\"huxtable\" + 0.005*\"vilsack\" + 0.005*\"nassau\" + '\n",
      "  '0.005*\"anthropologist\" + 0.004*\"suspense\" + 0.004*\"casa\" + 0.004*\"mota\" + '\n",
      "  '0.003*\"gran\" + 0.003*\"austen\"'),\n",
      " (1,\n",
      "  '0.010*\"yin\" + 0.010*\"fa\" + 0.006*\"kynect\" + 0.005*\"quicksand\" + '\n",
      "  '0.005*\"steinberg\" + 0.005*\"staging\" + 0.004*\"baxter\" + 0.004*\"goodell\" + '\n",
      "  '0.004*\"overstay\" + 0.004*\"twentyonestcentury\"'),\n",
      " (2,\n",
      "  '0.007*\"qing\" + 0.006*\"konnikova\" + 0.005*\"vinegar\" + 0.005*\"condominium\" + '\n",
      "  '0.004*\"tomlin\" + 0.004*\"rhoade\" + 0.004*\"snowflake\" + 0.004*\"vividly\" + '\n",
      "  '0.003*\"eliot\" + 0.003*\"doubly\"'),\n",
      " (3,\n",
      "  '0.009*\"mckinley\" + 0.008*\"chi\" + 0.008*\"ni\" + 0.008*\"edu\" + 0.007*\"napoli\" '\n",
      "  '+ 0.005*\"lai\" + 0.005*\"nast\" + 0.005*\"conde\" + 0.005*\"serenity\" + '\n",
      "  '0.005*\"barrow\"'),\n",
      " (4,\n",
      "  '0.008*\"tai\" + 0.008*\"shu\" + 0.006*\"fourteenseven\" + 0.006*\"hao\" + '\n",
      "  '0.005*\"barrack\" + 0.004*\"sundays\" + 0.004*\"fortyfiveth\" + 0.004*\"yong\" + '\n",
      "  '0.004*\"fortysixth\" + 0.004*\"paulson\"'),\n",
      " (5,\n",
      "  '0.012*\"ye\" + 0.010*\"cheng\" + 0.006*\"violet\" + 0.005*\"thatcher\" + 0.005*\"za\" '\n",
      "  '+ 0.004*\"bagel\" + 0.004*\"vonn\" + 0.004*\"avocado\" + 0.003*\"windsor\" + '\n",
      "  '0.003*\"sociology\"'),\n",
      " (6,\n",
      "  '0.004*\"lindner\" + 0.004*\"mahmoud\" + 0.003*\"ano\" + 0.003*\"kai\" + '\n",
      "  '0.003*\"venerable\" + 0.003*\"habia\" + 0.003*\"pepe\" + 0.003*\"opinionthe\" + '\n",
      "  '0.003*\"desde\" + 0.003*\"insulin\"'),\n",
      " (7,\n",
      "  '0.007*\"min\" + 0.005*\"poe\" + 0.004*\"boardwalk\" + 0.004*\"censure\" + '\n",
      "  '0.004*\"improbable\" + 0.004*\"travolta\" + 0.004*\"implicitly\" + '\n",
      "  '0.003*\"videotape\" + 0.003*\"sturdy\" + 0.003*\"rawlence\"'),\n",
      " (8,\n",
      "  '0.008*\"atletico\" + 0.007*\"tong\" + 0.007*\"ling\" + 0.005*\"alfredo\" + '\n",
      "  '0.004*\"bulgaria\" + 0.004*\"kang\" + 0.004*\"stamford\" + 0.004*\"globalization\" '\n",
      "  '+ 0.003*\"springsteen\" + 0.003*\"playhouse\"'),\n",
      " (9,\n",
      "  '0.011*\"xing\" + 0.010*\"wei\" + 0.010*\"xin\" + 0.009*\"jian\" + 0.009*\"bu\" + '\n",
      "  '0.009*\"xiang\" + 0.009*\"ying\" + 0.009*\"jing\" + 0.008*\"xian\" + 0.008*\"zai\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "# Saving model in a pickle file\n",
    "models_path = \"../assets/models\"\n",
    "lda_model_path = os.path.abspath(os.path.join(models_path, \"2020-09-21_10-topics.p\"))\n",
    "pickle.dump(lda_model, open(lda_model_path, \"wb\"))\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:46:11.739579Z",
     "start_time": "2020-09-22T11:43:27.442529Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heladio/PycharmProjects/trend-forecast/venv/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c6c12ab99b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Compute Perplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Perplexity: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# a measure of how good the model is. lower the better.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Compute Coherence Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/trend-forecast/venv/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mlog_perplexity\u001b[0;34m(self, chunk, total_docs)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mperwordbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         logger.info(\n\u001b[1;32m    823\u001b[0m             \u001b[0;34m\"%.3f per-word bound, %.1f perplexity estimate based on a held-out corpus of %i documents with %i words\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/trend-forecast/venv/lib/python3.8/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mbound\u001b[0;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m             \u001b[0mElogthetad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mgammad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=df.text, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:31:33.500417Z",
     "start_time": "2020-09-22T11:31:33.464815Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_lda = lda_model[filterred_df.corpus.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T11:36:44.061026Z",
     "start_time": "2020-09-22T11:36:44.055775Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heladio/PycharmProjects/trend-forecast/venv/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
